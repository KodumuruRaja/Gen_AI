{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f757c16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os \n",
    "from langchain.document_loaders import PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from flask import Flask, request, jsonify\n",
    "import requests\n",
    "from datetime import date\n",
    "import requests\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/test', methods=['POST'])\n",
    "def analyze():\n",
    "    \n",
    "    \n",
    "    #ProxyCurl API and End POints (These are free and have limited balance, if expired use your own)\n",
    "    api_endpoint = 'https://nubela.co/proxycurl/api/v2/linkedin'\n",
    "    linkedin_profile_url = request.json.get(\"linkedin_profile_url\",None)\n",
    "    #linkedin_profile_url = 'https://www.linkedin.com/in/kodumururaja/'\n",
    "    api_key = 'IVTeWufdpL6NoZsWbHGG4Q'\n",
    "    headers = {'Authorization': 'Bearer ' + api_key}\n",
    "\n",
    "    response = requests.get(api_endpoint,\n",
    "                            params={'url': linkedin_profile_url,'skills': 'include'},\n",
    "                            headers=headers)\n",
    "    profile_data = response.json()\n",
    "\n",
    "    api_endpoint = 'https://nubela.co/proxycurl/api/linkedin/job'\n",
    "    \n",
    "    job_post_url = request.json.get(\"job_post_url\",None)\n",
    "    #job_post_url = 'https://www.linkedin.com/jobs/view/3832463995/?refId=52774a50-d67f-4dc6-b9a5-5edc73dbff5d&trackingId=0WmNycKJSc23mUn%2Bk0gqqg%3D%3D'\n",
    "\n",
    "    params = {\n",
    "        'url': job_post_url,\n",
    "    }\n",
    "    response1 = requests.get(api_endpoint,\n",
    "                            params=params,\n",
    "                            headers=headers)\n",
    "    job_data = response1.json()\n",
    "    \n",
    "    # Converting the skills to a string with a meaningful prompt attached at the begining.\n",
    "\n",
    "    q1 = \", \".join(profile_data['skills'])\n",
    "    q1 = \"The skills of the candidate are\"+\" \"+ q1\n",
    "    \n",
    "    # finding the number of days to find the relevant number of years experience of the candidate w.r.t the start and end dates of his companies. \n",
    "\n",
    "    total_days = 0\n",
    "    exp = profile_data['experiences']\n",
    "    for i in range(len(exp)):\n",
    "        start_date = date(exp[i]['starts_at']['year'], exp[i]['starts_at']['month'], exp[i]['starts_at']['day'])\n",
    "        #print(start_date)\n",
    "\n",
    "        if exp[i]['ends_at'] is None:\n",
    "            end_date = date.today()  # today's date\n",
    "        else:\n",
    "            end_date = date(exp[i]['ends_at']['year'], exp[i]['ends_at']['month'], exp[i]['ends_at']['day'])\n",
    "        #print(end_date)\n",
    "        #print((date(end_date.year,end_date.month,end_date.day)-date(start_date.year,start_date.month,start_date.day)).days)\n",
    "        total_days+= (date(end_date.year,end_date.month,end_date.day)-date(start_date.year,start_date.month,start_date.day)).days\n",
    "    #print(\"total experience in years is {}\".format(round((total_days/365),2)))\n",
    "    total_exp = round((total_days/365),2)\n",
    "    total_exp = \"The total experience of the candidate is {} years\".format(total_exp)\n",
    "    \n",
    "    # Mapping the Experience level to a numerical value and adding a prompt at the begining.\n",
    "\n",
    "    job_exp = job_data[\"seniority_level\"]\n",
    "    expl = {\"Entry level\":\"0-2\",\"Associate\":\"2-4\",\"Mid-Senior level\":\"4-7\"}\n",
    "    for i in expl.keys():\n",
    "        if job_exp == i:\n",
    "            job_exp = expl[i]\n",
    "    job_exp = \"The experience required for the job is in the range of {} years\".format(job_exp)\n",
    "    \n",
    "    # OpenAI API Key\n",
    "    os.environ[\"OPENAI_API_KEY\"] = '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'\n",
    "\n",
    "    persist_directory = \"./storage\"\n",
    "    #pdf_path = \"./docs/i1040gi.pdf\"\n",
    "    \n",
    "    text = job_data[\"job_description\"]\n",
    "    \n",
    "    # creating a txt file\n",
    "\n",
    "    with open(\"jd.txt\", 'w', encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(text + q1 + job_exp + total_exp)\n",
    "        \n",
    "    # Loding the txt file\n",
    "    \n",
    "    loader = PyMuPDFLoader(\"jd.txt\")\n",
    "    documents = loader.load()\n",
    "    \n",
    "    #Splitting text using RecursiveCharacterTextSplitte\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=10)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # OpenAI Embeddings\n",
    "\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    #Chroma Vector DB\n",
    "    vectordb = Chroma.from_documents(documents=texts, \n",
    "                                     embedding=embeddings,\n",
    "                                     persist_directory=persist_directory)\n",
    "    vectordb.persist()\n",
    "\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "    \n",
    "    # Chat Model\n",
    "    llm = ChatOpenAI(model_name='gpt-3.5-turbo-1106')\n",
    "\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "\n",
    "    user_input = \"based on the job description and the skills of the candidate suggest the skills required to learn, and check the experience range required for the job to the candidate experience and suggest wether he had right experience or will he need to gain enough experience, and mention the number of years to gain. If candidate had more experience than required then mention him that he had enough experience.\"\n",
    "\n",
    "    query = f\"###Prompt {user_input}\"\n",
    "    try:\n",
    "        llm_response = qa(query)\n",
    "        #print(llm_response[\"result\"])\n",
    "        return jsonify({\"answer\": llm_response[\"result\"]})\n",
    "    except Exception as err:\n",
    "        #print('Exception occurred. Please try again', str(err))\n",
    "        return jsonify({\"error\": str(e)})\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfaaf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# Gen_AI_RajaKodumuru

1) I am successfully and able to retrieve the job description page, but after testing the code by sending 3-4 url's automatically Linkedin security system caught it and denied my request, so right now I am not able to fetch the data using BeautifulSoup.
2) Linkedin Security System doesn't allows us to scrap the data and it detects our pc details as well.
3) To scrap Linkedin Profiles and job description page we definitely need to use an api key, without api when i tried it didn't work, Linkedin profile page directly denied.
4) Scrapping using Chrome Driver is not efficient, and we need to downgrade the browser, each and every user have their browser updated in their mobiles and it is not efficient technique to use
5) I had scrapped Profiles and job description page using ProxyCurl API, it is successful.
6) I had been writing code using LangChain, GPT3.5 API key with ChatOpenAI, I have tested multiple ways but this is the efficient one.

**Code Explanation**
1) The Flask API takes Candidate Profile Url and Job Description Page Url as Input.
2) It gives the output to upgrade the necessary skills and suggests to gain required number of enough experience if there is any gap between the actual and required.
3) Here we are using ProxyCurl two end points, one for scrapping profiles and one to scrap job description pages.
4) both the Linkedin Profiles and job description pages data are scrapped in the json format.
5) I had retrieved all the skills of the candidate and made a string begining with a prompt.
6) Retrieved experience from the job description page and assigned meaningful range of numerical experience and mapped it with candidate profile experience by calculating the candidate experience using python's datetime.
7) after that inserted all the text to a pdf file and did text splitting, and embedding.
8) Stored texts and embeddings into Chroma Vector DB.
9) Used LangChain Chat Model, ChatOpenAI, sent a prompt to retrieve desired ouput.
10) AT last successful in getting the output.
